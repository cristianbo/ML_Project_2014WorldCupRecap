\documentclass{article} % For LaTeX2e
\usepackage{CJK}
\usepackage{561project,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{booktabs}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09

\newcommand{\beq}{\vspace{0mm}\begin{equation}}
\newcommand{\eeq}{\vspace{0mm}\end{equation}}
\newcommand{\beqs}{\vspace{0mm}\begin{eqnarray}}
\newcommand{\eeqs}{\vspace{0mm}\end{eqnarray}}
\newcommand{\barr}{\begin{array}}
\newcommand{\earr}{\end{array}}
\newcommand{\Amat}[0]{{{\bf A}}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}[0]{{{\bf E}}}
\newcommand{\Fmat}[0]{{{\bf F}}\xspace}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}[0]{{{\bf J}}\xspace}
\newcommand{\Kmat}[0]{{{\bf K}}\xspace}
\newcommand{\Lmat}[0]{{{\bf L}}}
%\newcommand{\Mmat}[0]{{{\bf M}}\xspace}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}[0]{{{\bf N}}\xspace}
\newcommand{\Omat}[0]{{{\bf O}}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}[0]{{{\bf Q}}\xspace}
\newcommand{\Rmat}[0]{{{\bf R}}}
\newcommand{\Smat}[0]{{{\bf S}}}
\newcommand{\Tmat}[0]{{{\bf T}}}
\newcommand{\Umat}[0]{{{\bf U}}}
\newcommand{\Vmat}[0]{{{\bf V}}}
\newcommand{\Wmat}[0]{{{\bf W}}}
\newcommand{\Xmat}[0]{{{\bf X}}}
\newcommand{\Ymat}{{\bf Y}}
%\newcommand{\Ymat}[0]{{{\bf Z}}}
\newcommand{\Zmat}{{\bf Z}}

\newcommand{\av}[0]{{\boldsymbol{a}}}
\newcommand{\bv}[0]{{\boldsymbol{b}}}
\newcommand{\cv}[0]{{\boldsymbol{c}}}
\newcommand{\dv}{\boldsymbol{d}}
\newcommand{\ev}[0]{{\boldsymbol{e}}\xspace}
\newcommand{\fv}[0]{{\boldsymbol{f}}\xspace}
\newcommand{\gv}[0]{{\boldsymbol{g}}\xspace}
\newcommand{\hv}[0]{{\boldsymbol{h}}\xspace}
\newcommand{\iv}[0]{{\boldsymbol{i}}\xspace}
\newcommand{\jv}[0]{{\boldsymbol{j}}\xspace}
\newcommand{\kv}[0]{{\boldsymbol{k}}\xspace}
\newcommand{\lv}[0]{{\boldsymbol{l}}}
\newcommand{\mv}[0]{{\boldsymbol{m}}}
\newcommand{\nv}[0]{{\boldsymbol{n}}\xspace}
\newcommand{\ov}[0]{{\boldsymbol{o}}\xspace}
\newcommand{\pv}[0]{{\boldsymbol{p}}}
\newcommand{\qv}[0]{{\boldsymbol{q}}\xspace}
\newcommand{\rv}{\boldsymbol{r}}
\newcommand{\sv}[0]{{\boldsymbol{s}}}
\newcommand{\tv}[0]{{\boldsymbol{t}}\xspace}
\newcommand{\uv}{\boldsymbol{u}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\wv}{\boldsymbol{w}}
\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\zv}{\boldsymbol{z}}
\newcommand{\cdotv}{\boldsymbol{\cdot}}

\newcommand{\Gammamat}[0]{{\boldsymbol{\Gamma}}\xspace}
\newcommand{\Deltamat}[0]{{\boldsymbol{\Delta}}\xspace}
\newcommand{\Thetamat}{\boldsymbol{\Theta}}
\newcommand{\Betamat}{\boldsymbol{\Beta}}
\newcommand{\Lambdamat}{\boldsymbol{\Lambda}}
\newcommand{\Ximat}[0]{{\boldsymbol{\Xi}}\xspace}
\newcommand{\Pimat}[0]{{\boldsymbol{\Pi}}\xspace}
\newcommand{\Sigmamat}[0]{{\boldsymbol{\Sigma}}}
\newcommand{\Upsilonmat}[0]{{\boldsymbol{\Upsilon}}\xspace}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\Psimat}{\boldsymbol{\Psi}}
\newcommand{\Omegamat}[0]{{\boldsymbol{\Omega}}}

\newcommand{\alphav}{\boldsymbol{\alpha}}
\newcommand{\betav}[0]{{\boldsymbol{\beta}}}
\newcommand{\gammav}[0]{{\boldsymbol{\gamma}}\xspace}
\newcommand{\deltav}[0]{{\boldsymbol{\delta}}\xspace}
\newcommand{\epsilonv}{\boldsymbol{\epsilon}}
\newcommand{\zetav}[0]{{\boldsymbol{\zeta}}\xspace}
\newcommand{\etav}[0]{{\boldsymbol{\eta}}\xspace}
\newcommand{\ellv}[0]{{\boldsymbol{\ell}}}
\newcommand{\thetav}{\boldsymbol{\theta}}
\newcommand{\iotav}[0]{{\boldsymbol{\iota}}}
\newcommand{\kappav}[0]{{\boldsymbol{\kappa}}\xspace}
\newcommand{\lambdav}[0]{{\boldsymbol{\lambda}}}
\newcommand{\muv}[0]{{\boldsymbol{\mu}}}
\newcommand{\nuv}[0]{{\boldsymbol{\nu}}}
\newcommand{\xiv}[0]{{\boldsymbol{\xi}}\xspace}
\newcommand{\omicronv}[0]{{\boldsymbol{\omicron}}\xspace}
\newcommand{\piv}{\boldsymbol{\pi}}
\newcommand{\rhov}[0]{{\boldsymbol{\rho}}\xspace}
\newcommand{\sigmav}[0]{{\boldsymbol{\sigma}}\xspace}
\newcommand{\tauv}[0]{{\boldsymbol{\tau}}\xspace}
\newcommand{\upsilonv}[0]{{\boldsymbol{\upsilon}}\xspace}
\newcommand{\phiv}{\boldsymbol{\phi}}
\newcommand{\chiv}[0]{{\boldsymbol{\chi}}\xspace}
\newcommand{\psiv}{\boldsymbol{\psi}}
\newcommand{\varthetav}{\boldsymbol{\vartheta}}
\newcommand{\omegav}[0]{{\boldsymbol{\omega}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Dcal}{\mathcal{D}}

\title{Bayesian Probabilistic Models \\ for 2014 FIFA World Cup Prediction}

\author{
Zhe Gan(zg27), Yunchen Pu(yp42), Zhao Song(zs42) \\
Department of ECE, \ Duke University }

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy

\begin{document}

\bibliographystyle{plain}
\maketitle

\begin{abstract}
We develop two Bayesian approaches to predict the 2014 FIFA World Cup, by applying Bradley-Terry model and Bayesian linear regression model with data augmentation respectively. For each model, we develop the Gibbs sampler to perform the posterior inference. We apply our algorithms on the different datasets and the results show that we could always obtain convincing predictions.
\end{abstract}

\section{Introduction}

In our project, we aim to investigate how the use of a probabilistic model might be able to predict the results for the 2014 FIFA World Cup. By treating each game played as a pairwise comparison experiment, we can use Bradley-Terry model \cite{bradley1952rank} to fit the data. Various extensions have been developed to handle home advantage \cite{agresti1990categorical}, ties \cite{rao1967ties}, multiple \cite{plackett1975the,luce1959individual}and team comparisons \cite{huang2006generalized}. It is possible to implement a simple iterative procedure \cite{zermelo1929die} to get the maximum likelihood solution. This is proved to be a special case of a more general class of algorithms called MM algorithms \cite{lange2000opt,hunter2004mm}. Recently, Bayesian inference has been proposed to deal with this problem. In \cite{guiver2009bayesian}, an efficient Expectation-Propagation method is developed; in \cite{gormley2009a}, a carefully designed MH algorithm is proposed, while in \cite{caron2012efficient}, by introducing latent variables, the author tells us by deriving simple Gibbs sampler, this problem can be efficiently solved.

There have been many other prior works to model the soccer games statistically \cite{albert2005anthology,baio2010bayesian,lee1997modeling,rue2000prediction}. The Bayesian hierarchical model is employed in \cite{baio2010bayesian} to model the number of goals scored by two teams in each match; the authors there used the Poisson-LogNormal model to fit the data. In \cite{lee1997modeling}, the author proposed a simplified generalized linear model and apply it on the final rank analysis. The \cite{rue2000prediction} developed a Bayesian dynamic generalized linear model to describe the relationship between the outcomes of soccer matches (Win, Tie, and Loss) and the explanatory variables: they employed the MCMC approach to fit the model and make the prediction. However, the above references have the following limitations: \cite{baio2010bayesian} did not use match statistics other than number of goals in their model. \cite{lee1997modeling} is a \emph{simplified} version of generalized linear model. \cite{rue2000prediction} only estimated two variables to do the inference; however, there are more match statistics available to use for the inference.

In our project, we have developed two probabilistic models to fit our data. In section 2, we introduce these two models. In section 3, we derive posterior inference in detail. In section 4, we implement our Gibbs sampler on the soccer data we have collected. Finally, in section 5 we get our conclusion.

\section{Model}

In this section, we propose two models separately to predict the ranks from previous soccer dataset.

\subsection{Bradley-Terry Model with ties}

Suppose the only data we have access to is the win-loss data. Consider the situation that $n$ teams are repeatedly compared with each other in pairs. For any two teams $i$ and $j$, we use the following model \cite{rao1967ties}
\beq \mbox{Pr}(i \ \mbox{beats} \ j) = \frac{\lambda_i}{\lambda_i+\theta\lambda_j} \eeq
\beq \mbox{Pr}(i \ \mbox{ties} \ j) = \frac{(\theta^2-1)\lambda_i\lambda_j}{(\lambda_i+\theta\lambda_j)(\theta\lambda_i+\lambda_j)} \eeq
where $\theta>1$, and $\lambda_i$ is a latent positive-valued strength parameter that we have to do inference on and we denote $\lambdav:=\{\lambda_i\}_{i=1}^n$. We assign a prior to $\lambdav$ and $\theta$ such that
\beq \lambda_i \sim \mbox{Gamma}(a,b),\ \hat{\theta} \sim \mbox{Gamma} (c,d) \ \mbox{where} \ \hat{\theta}=\theta-1\eeq

\subsection{Bayesian Linear Regression with Data Augmentation}

We aim to establish the relationship between team ranks $\boldmath{y} = [y_1, \, y_2, \ldots, y_n]^T \in \mathbb{Z}^n$ and team statistics $\boldmath{X} = [\boldmath{x}_1, \, \boldmath{x}_2, \, \ldots, \boldmath{x}_p ] \in \mathbb{R}^{n \times p}$ whose columns correspond to specific statistics. Since $\boldmath{y}$ is the categorical data, we propose to use the following data augmentation approach to model the data \cite{Hoff}: For $i = 1, \, 2, \, \ldots, \, n$,	
\begin{subequations}
\begin{equation}
z_i \sim \mathcal{N} (z_i; \, X_i^T \betav, 1), \ y_i = \sum_{j = 1}^d \, j \, \mathbbm{1} \{ z_i \in (\tau_{j - 1}, \tau_j) \}
\end{equation}
where $z_i$ is the latent variable, $j \in \{1, \, 2, \, \ldots, d \}$ is the categorical variable, and $\tau_j$ refers to the threshold. Moreover, we assign the following prior on $\mathbb{\beta}$ and $\tau_j$:
\begin{eqnarray}
\pi (\boldmath{\beta}) &\sim& \mathcal{N} (\boldmath{\beta}; \boldmath{\beta}_0, \, \Sigma ) \\
\pi (\tau_1, \, \tau_2, \, \ldots, \tau_d) &\sim& \mathbbm{1}
( - \infty < \tau_1 < \cdots < \tau_d < \infty)
\end{eqnarray}
\end{subequations}

The fitted parameters $\hat{\boldmath{\beta}}$ should reflect the corresponding weights for each match statistic. We could employ the fitted $\hat{\boldmath{\beta}}$ to accomplish the prediction using new dataset $\tilde{\boldmath{X}}$ as $\tilde{\boldmath{z}} = \tilde{\boldmath{X}} \hat{\boldmath{\beta}}$.


\section{Posterior Inference}
\subsection{A Gibbs sampler for Bradley-Terry Model with ties}

For $1\leq i \not= j \leq n$ such that $s_{ij}>0$, let us introduce the latent variables $\Zmat=\{z_{ij}\}$ such that\cite{caron2012efficient}
\beq (z_{ij}|\lambda_i,\lambda_j,\theta) \sim \mbox{Gamma}(s_{ij},\lambda_{i}+\theta \lambda_{j})  \eeq
Hence, now we can get our complete log-likelihood as follow
\beq L_c(\Dcal,\Zmat|\lambdav,\theta) = T\log(\theta^2-1)+\sum_{1\leq i \not= j \leq n} \left[ s_{ij}\log \lambda_i-(\lambda_i+\theta \lambda_j)z_{ij}+(s_{ij}-1)\log z_{ij} \right] + \mbox{const}  \eeq
where $\omega_{ij}$ denotes the number of times team $i$ beats team $j$, $t_{ij}=t_{ji}$ is the number of ties between team $i$ and team $j$ and $s_{ij}=\omega_{ij}+t_{ij}$. We assume $\omega_{ii}=t_{ii}=0$ by convention. Furthermore, $T = \frac{1}{2}\sum_{1\leq i \not= j \leq n} t_{ij}$ is the total number of ties.

Combining the priors and likelihood together, we can sample from the posterior distribution of $\lambdav,\theta,\Zmat$ using Gibbs sampler as follows at iteration $s$. Here, we use $p(\cdot|-)$ to denote the conditional posterior of one variable given on all others.

\textbf{Sampling} $z_{ij}$ which satisfies $1\leq i \not= j \leq n$ and $s_{ij}>0$ from
\beq p(z_{ij}^{(s)}|-) = \mbox{Gamma}\left(s_{ij},\lambda_i^{(s-1)}+\theta^{(s-1)}\lambda_j^{(s-1)}\right) \eeq
\textbf{Sampling} $\{\lambda_i\}_{i=1}^{n}$ from
\beq p(\lambda_i^{(s)}|-) = \mbox{Gamma}\left(a+\sum_{j\not=i} s_{ij}, b+\sum_{j\not=i} (z_{ij}^{(s)}+\theta^{(s-1)}z_{ji}^{(s)}) \right)  \eeq
\textbf{MH update} $\theta$ using a normal random walk to propose a new $\theta^*$, and accept $\theta^*$ with probability
\beq \min \left\{ 1,\frac{p(\theta^*|-)}{p(\theta^{(s-1)}|-)}  \right\} \eeq
where
\beq p(\theta|-)\propto (\theta^2-1)^T (\theta-1)^{c-1} \exp \left[ -\left( d+\sum_{1\leq i \not= j \leq n} \lambda_j z_{ij} \right)\theta \right]\mathbb{I}(\theta>1)  \eeq
Note that the likelihood is invariant to a rescaling of $\lambdav$, hence $\lambdav$ is not identifiable. To avoid identifiability problem, we can put an additional constraint $\Lambda=\sum_{i=1}^n \lambda_i =1$. What's more, $b=na$ such that the posterior mean estimation satisfy $\Lambda=1$. We can further put a prior on $a$ to estimate the value of $a$ from the data using MH update.

\subsection{A Gibbs sampler for Bayesian Linear Regression}

We could derive the Gibbs sampler as follows:

\textbf{Sampling} $\boldmath{\beta}$ from
\beq ( \betav | - ) \sim \mathcal{N} ( \betav; \hat{\betav}, \, \hat{\Sigma} ) \eeq
where
\begin{eqnarray*}
\boldmath{\hat{\beta}} &=& \hat{\Sigma} \, ( \Sigma^{-1} \boldmath{\beta_0} + \boldmath{X}^T \boldmath{z} ) \\
\hat{\Sigma} &=& ( \boldmath{X}^T \boldmath{X} + \Sigma )^{-1}
\end{eqnarray*}
\textbf{Sampling} $z_i$ for $i = 1, \, 2, \, \ldots, \, n$ from
\beq (z_i | - ) \sim \mathcal{N} ( z_i; X_i^T \boldmath{\beta}, 1 ) \, \mathbbm{1} \{ z_i \in ( \tau_{y_i - 1}, \tau_{y_i} ) \} \eeq
\textbf{Sampling} $\tau_j$ for $j = 1, \, 2, \, \ldots, \, d$ from
\beq
(\tau_j | - ) \sim \text{Unif} \left( \max \{z_i, \text{such that } y_i = j \}, \,  \min \{z_i, \text{such that } y_i = j+1 \} \right)
\eeq

\section{Experimental Results}
\subsection{Bradley-Terry Model with ties}

We have collected soccer game results from 2008 to 2013, consisting of 1,158 matches between 133 national teams. Gibbs samplers were initialized at $(\lambda_1^{(0)},\cdots,\lambda_n^{(0)})=(\frac{1}{133},\cdots,\frac{1}{133})$ and $\theta^{(0)}=1.5$. We set $c=1, d=0, b = na$ and we learn the $a$ value from the data. The Gibbs sampler were run with 25,000 iterations with 5,000 burn-ins.

We can get our ranking results of the qualified teams for the 2014 FIFA World Cup shown in Table \ref{Tab:rank} (we only show the first 8th teams). The approximated posterior is shown in Figure \ref{Fig:rankresult}. We can see clearly that Spain has a high probability that ranks first, while it is hard to decide whether Germany or Netherland ranks second. However, what we can guarantee is that both Germany and Netherland have a high probability to rank before other 4 teams. It is also difficult to rank the last 4 teams, since their posterior overlaps too much. This is because our sample size is too small compared with the number of teams we have.
\begin{table}[h]
\caption{\small Ranking of 32 qualified teams for FIFA World Cup 2014.} \label{Tab:rank}
\begin{center}\begin{small}
\begin{tabular}{lll}
{\bf Rank}  &{\bf FIFA Ranking} &{\bf BT Model Ranking}  \\
\hline \\
1  &  Spain         & Spain        \\
2  &  Germany       & Netherlands   \\
3  &  Argentina     & Germany       \\
4  &  Colombia      & Brazil        \\
5  &  Portugal      & Argentina     \\
6  &  Uruguay       & England       \\
7  &  Italy         & Bosnia-Herzegovina  \\
8  &  Switzerland   & Portugal      \\
\hline
\end{tabular}\end{small}
\end{center}
\end{table}
\begin{figure}[h]
\centering
\includegraphics[width=2in]{rank.png}
\caption{Approximated posterior using MCMC}\label{Fig:rankresult}
\end{figure}

Now, suppose we want to do prediction. Based on the 544 match results from 2008 to 2009, we predict the results of each match played in the 2010 FIFA World Cup. Each match has three possible results $E_i$: win(+1), tie(+0.5) and loss(0). We can get the accuracy rate equals to 60.94\% and the RMSE equals to 37.49\%. The posterior mean of $\theta$ and $a$ is 1.8698 and 1.0070 respectively. If we just guess randomly, we can only get accuracy equal to $1/3$, using the model proposed here can greatly improve prediction accuracy.

\subsection{Bayesian Linear Regression}

The predictors we employed in our model fitting and prediction parts are goals for (GF), goals against(GA), shots on goal (SOG), tackles (TCK), and passes completed (PC).

We first run the Gibbs sampler on the data from $2010$ FIFA World Cup. The predictors corresponding to teams with rank $1, \, 3, \, \ldots, 31$ are used to fit the model and the predictors corresponding to teams with rank $2, \, 4, \, \ldots, 32$ are used to check the model. The real rank for those $16$ teams is:

\texttt{NED URU BRA PAR CHI USA MEX SVK SLO RSA NZL DEN ITA ALG HON PRK}

and the estimated rank based on our posterior mean is

\texttt{BRA NED URU PAR USA SLO ITA MEX NZL CHI RSA SVK DEN ALG HON PRK}

We can see our algorithm has relatively good prediction performance for the teams with high/low ranks while relatively poor prediction performance for the teams with middle ranks. Table~\ref{tab:beta_est} shows the posterior means of each regression coefficients from $10,000$ samples (after $5,000$ burn-in).
\begin{table*}[t]
\centering
\caption{Posterior mean of regression coefficients from Gibbs sampler.}
\begin{tabular}{ c c c c c c}
%\centering
& GF & GA & SOG & TCK  & PC  \\
\midrule
& 0.9290 & -0.7212 & 0.5695 &
-0.1033 & 0.4418
\end{tabular}
\label{tab:beta_est}
\end{table*}

Next, we apply our fitted model from $2012$ UEFA Euro dataset and $2011$ Copa America dataset to predict the performances of European and South America teams in the $2014$ FIFA World Cup, respectively. Figure ~\ref{fig:pred} shows the mean values of predicted latent variable for each team. We could observe that Spain is most likely to win the Word Cup among all the European teams listed while Germany, Russia, and Italy are also the strong championship candidates. For the teams in South America, Argentina is predicted to have the largest chance to be the champion and Brazil and Uruguay are also the potential championship candidates.
\begin{figure}[h]
\centering
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{Pred_Euro.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{Pred_Copa.pdf}
}
\caption{The predicted latent value for each team in (a) Euro and (b) South America}
\label{fig:pred}
\end{figure}
\section{Conclusion}
Both of these two models introduce latent strength parameters for ranking and prediction. In the first model, we establish the relationship between team ranks and repeated comparison results. In the second model, we establish the relationship between team ranks and match statistics. For both methods, we implement the Gibbs sampler to perform the posterior inference. The experimental results tell us that the models can fit the data well and provide the reasonable prediction.

\bibliography{561project}

\end{document}
